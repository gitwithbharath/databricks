{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee4549b-d68d-452e-9761-156ed0e802ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = [ (1, \"HR\", 50000, \"2022-01-01\"), (2, \"IT\", 70000, \"2021-06-15\"), (3, \"HR\", 60000, \"2023-03-10\"), (4, \"IT\", 80000, \"2020-12-01\"), ] \n",
    "\n",
    "# Define schema and create DataFrame \n",
    "columns = [\"employee_id\", \"department\", \"salary\", \"join_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b4c011-1e37-40f8-b2b2-01dabdd5038f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491400a0-a431-41eb-b12c-76eb051bf3b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8e4757-7f20-45e8-8a97-c28a56801572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy(\"department\").orderBy(\"join_date\")\n",
    "df_rt = df.withColumn(\"Running_Total\", F.sum(F.col(\"salary\")).over(window)).withColumn(\"d_rank\", F.dense_rank().over(Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b21ec54f-6700-442f-b06e-f2edf7fd2011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_rt.show()\n",
    "display(df_rt)\n",
    "# df_rt.write.mode(\"overwrite\").saveAsTable(\"employee_rt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c47026dc-3136-4ace-b1fc-d3bbb6278822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write a PySpark script to implement a custom transformation that categorizes products based on their price. Add a new column price_category to the DataFrame with the following conditions:\n",
    "\n",
    "\"Low\" for price < 100\n",
    "\n",
    "\"Medium\" for 100 <= price < 500\n",
    "\n",
    "\"High\" for price >= 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c4da24-701e-406e-ae7f-8c2376186d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data2 = [(\"Laptop\", 800), (\"Mouse\", 25), (\"Keyboard\", 150), (\"Monitor\", 300)] \n",
    "columns2 = [\"product\", \"price\"] \n",
    "\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121ff699-ddc0-492c-af0f-1d93eb5fc53f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.withColumn(\"price_category\", F.when(df2.price >= 500, \"High\").when((df2.price >= 100) & (df2.price < 500), \"Medium\").otherwise(\"Low\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ce83d8d-550b-44f7-9dad-d23db014c4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "This new cell is from test branch"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
