{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c972c69b-f157-40d1-80b2-85d820f607ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ISO Date input format - 2024-10-27T13:00:00 (yyyy-MM-dd'T'HH:mm:ss)\n",
    "# Read date as input and get the day of week - like Sun, Mon, etc.,\n",
    "# create input_date widget\n",
    "\n",
    "dbutils.widgets.text(\"input_date\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0114d843-332b-486e-9820-3ac12b9213b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read value from widget\n",
    "\n",
    "_input_date = dbutils.widgets.get(\"input_date\")\n",
    "print(_input_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbc10acb-354f-4831-a80e-cdff784d2cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Format the date to extract the day of the week\n",
    "# set conf to LEGACY to support datetime patterns\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c820981-faf7-4f33-8242-d7bcc61165e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_input_day = spark.sql(f\"\"\"\n",
    "                       SELECT date_format(to_timestamp('{_input_date}', \"yyyy-MM-dd'T'HH:mm:ss\"), 'E')\n",
    "                       \"\"\").collect()[0][0]\n",
    "\n",
    "print(_input_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e252a7c8-e371-4ec3-94b9-9bded460e7a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Export the day of week using taskValues to use in next task\n",
    "dbutils.jobs.taskValues.set(\"input_day\", _input_day)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_get_run_day",
   "widgets": {
    "input_date": {
     "currentValue": " 2024-10-27T13:00:00",
     "nuid": "b170dd82-7309-4fe1-b9f2-e14c0ad07907",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": " ",
      "label": null,
      "name": "input_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": " ",
      "label": null,
      "name": "input_date",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
